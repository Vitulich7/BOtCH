# -*- coding: utf-8 -*-
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOtCH: улучшенный чат-бот с генеративной моделью DialoGPT\n",
    "В этом ноутбуке реализован бот с DialoGPT, базовая оценка качества и демонстрация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка нужных библиотек\n",
    "!pip install transformers torch nltk matplotlib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели и токенизатора DialoGPT (small для скорости)\n",
    "model_name = 'microsoft/DialoGPT-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция генерации ответа от DialoGPT\n",
    "def generate_response(history, user_input, max_length=1000):\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "    bot_input_ids = torch.cat([history, new_user_input_ids], dim=-1) if history is not None else new_user_input_ids\n",
    "    output_ids = model.generate(bot_input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "    response_ids = output_ids[:, bot_input_ids.shape[-1]:]\n",
    "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    return output_ids, response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример диалога с ботом\n",
    "history = None\n",
    "user_inputs = [\n",
    "    \"Привет!\",\n",
    "    \"Как тебя зовут?\",\n",
    "    \"Расскажи шутку\",\n",
    "    \"Спасибо, пока!\"\n",
    "]\n",
    "\n",
    "for user_input in user_inputs:\n",
    "    history, bot_response = generate_response(history, user_input)\n",
    "    print(f\"Пользователь: {user_input}\")\n",
    "    print(f\"БОТ: {bot_response}\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества (метрика BLEU) на тестовом наборе\n",
    "Для примера создадим простой тестовый набор вопросов и эталонных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Тестовый набор\n",
    "test_data = [\n",
    "    (\"Привет!\", \"Привет! Как я могу помочь?\"),\n",
    "    (\"Как тебя зовут?\", \"Я бот, меня зовут BOtCH.\"),\n",
    "    (\"Расскажи шутку\", \"Почему программисты любят природу? Потому что там много багов!\"),\n",
    "    (\"Пока\", \"До свидания!\")\n",
    "]\n",
    "\n",
    "bleu_scores = []\n",
    "for question, reference in test_data:\n",
    "    _, response = generate_response(None, question)\n",
    "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "    response_tokens = nltk.word_tokenize(response.lower())\n",
    "    score = sentence_bleu([reference_tokens], response_tokens)\n",
    "    bleu_scores.append(score)\n",
    "    print(f\"Вопрос: {question}\")\n",
    "    print(f\"Ответ бота: {response}\")\n",
    "    print(f\"Эталон: {reference}\")\n",
    "    print(f\"BLEU: {score:.3f}\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация результатов BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(range(len(bleu_scores)), bleu_scores, tick_label=[q for q, _ in test_data])\n",
    "plt.title('BLEU scores for test questions')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
