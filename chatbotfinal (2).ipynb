{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnUSF3OAyrWe"
      },
      "outputs": [],
      "source": [
        "# BOtCH: улучшенный чат-бот с генеративной моделью DialoGPT\n",
        "В этом ноутбуке реализован бот с DialoGPT, базовая оценка качества и демонстрация.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch nltk matplotlib --quiet\n"
      ],
      "metadata": {
        "id": "4ABwh-ncywhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "8qApwPzEyy_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'microsoft/DialoGPT-small'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Hrlutygby1Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(history, user_input, max_length=1000):\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "    bot_input_ids = torch.cat([history, new_user_input_ids], dim=-1) if history is not None else new_user_input_ids\n",
        "    output_ids = model.generate(bot_input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
        "    response_ids = output_ids[:, bot_input_ids.shape[-1]:]\n",
        "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "    return output_ids, response_text\n"
      ],
      "metadata": {
        "id": "yyvfQ3M2y2u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = None\n",
        "user_inputs = [\n",
        "    \"Привет!\",\n",
        "    \"Как тебя зовут?\",\n",
        "    \"Расскажи шутку\",\n",
        "    \"Спасибо, пока!\"\n",
        "]\n",
        "\n",
        "for user_input in user_inputs:\n",
        "    history, bot_response = generate_response(history, user_input)\n",
        "    print(f\"Пользователь: {user_input}\")\n",
        "    print(f\"БОТ: {bot_response}\")\n",
        "    print('---')\n"
      ],
      "metadata": {
        "id": "xn3xhCjly4uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # если не скачано\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Загружаем модель и токенизатор (пример с DialoGPT small)\n",
        "model_name = \"microsoft/DialoGPT-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "def generate_response_with_metrics(tokenizer, model, chat_history_ids, user_input, reference=None):\n",
        "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "    if chat_history_ids is not None:\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_input_ids\n",
        "\n",
        "    start = time.time()\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "    end = time.time()\n",
        "    gen_time = end - start\n",
        "\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    response_length = len(response)\n",
        "\n",
        "    bleu_score = None\n",
        "    if reference is not None:\n",
        "        smoothie = SmoothingFunction().method4\n",
        "        bleu_score = sentence_bleu([nltk.word_tokenize(reference.lower())], nltk.word_tokenize(response.lower()), smoothing_function=smoothie)\n",
        "\n",
        "    return chat_history_ids, response, gen_time, response_length, bleu_score\n",
        "\n",
        "test_data = [\n",
        "    (\"Привет!\", \"Привет! Как я могу помочь?\"),\n",
        "    (\"Как тебя зовут?\", \"Я бот, меня зовут BOtCH.\"),\n",
        "    (\"Расскажи шутку\", \"Почему программисты любят природу? Потому что там много багов!\"),\n",
        "    (\"Пока\", \"До свидания!\")\n",
        "]\n",
        "\n",
        "bleu_scores = []\n",
        "chat_history_ids = None  # для хранения контекста, если нужно\n",
        "\n",
        "for question, reference in test_data:\n",
        "    chat_history_ids, response, gen_time, resp_len, bleu = generate_response_with_metrics(tokenizer, model, chat_history_ids, question, reference)\n",
        "    bleu_scores.append(bleu)\n",
        "    print(f\"Вопрос: {question}\")\n",
        "    print(f\"Ответ бота: {response}\")\n",
        "    print(f\"Эталон: {reference}\")\n",
        "    print(f\"BLEU: {bleu:.3f}\")\n",
        "    print(f\"Время генерации: {gen_time:.3f} сек\")\n",
        "    print(f\"Длина ответа: {resp_len} символов\")\n",
        "    print('---')\n"
      ],
      "metadata": {
        "id": "gsvUDUpey6zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Функция генерации с метриками\n",
        "def generate_response_with_metrics(tokenizer, model, chat_history_ids, user_input, reference=None):\n",
        "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "    if chat_history_ids is not None:\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_input_ids\n",
        "\n",
        "    start = time.time()\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "    end = time.time()\n",
        "    gen_time = end - start\n",
        "\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    response_length = len(response)\n",
        "\n",
        "    bleu_score = None\n",
        "    if reference is not None:\n",
        "        # Разбиваем на слова, считаем BLEU с сглаживанием\n",
        "        smoothie = SmoothingFunction().method4\n",
        "        bleu_score = sentence_bleu([reference.split()], response.split(), smoothing_function=smoothie)\n",
        "\n",
        "    return chat_history_ids, response, gen_time, response_length, bleu_score\n",
        "\n",
        "# Пример вызова (reference — эталонный ответ, если есть)\n",
        "chat_history_ids, bot_response, gen_time, resp_len, bleu = generate_response_with_metrics(tokenizer, model, chat_history_ids, user_input, reference=\"эталонный ответ\")\n",
        "\n",
        "print(f\"Ответ: {bot_response}\")\n",
        "print(f\"Время генерации: {gen_time:.2f} сек\")\n",
        "print(f\"Длина ответа: {resp_len} символов\")\n",
        "if bleu is not None:\n",
        "    print(f\"BLEU: {bleu:.4f}\")\n"
      ],
      "metadata": {
        "id": "2EJ6uifR1z11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(range(len(bleu_scores)), bleu_scores, tick_label=[q for q, _ in test_data])\n",
        "plt.title('BLEU scores for test questions')\n",
        "plt.ylabel('BLEU score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jU-0Zep7y83V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}