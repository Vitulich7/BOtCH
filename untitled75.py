# -*- coding: utf-8 -*-
"""Untitled75.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12fyqNt3QUKQl7069zZhHzF7RJ_5NBkqg
"""

!pip install transformers torch streamlit --quiet

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time

# –ö—ç—à–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –∫–∞–∂–¥—ã–π —Ä–∞–∑
@st.cache(allow_output_mutation=True)
def load_model(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return tokenizer, model

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ DialoGPT
model_options = {
    "DialoGPT small": "microsoft/DialoGPT-small",
    "DialoGPT medium": "microsoft/DialoGPT-medium",
    "DialoGPT large": "microsoft/DialoGPT-large"
}

# –ü—Ä–æ—Å—Ç–µ–π—à–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã (–ø—Ä–∏–º–µ—Ä)
custom_templates = {
    "–û–±—ã—á–Ω—ã–π": lambda text: text,
    "–í–µ–∂–ª–∏–≤—ã–π": lambda text: "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, " + text,
    "–Æ–º–æ—Ä–Ω–æ–π": lambda text: text + " üòÑ"
}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
if "history" not in st.session_state:
    st.session_state.history = None
if "chat_log" not in st.session_state:
    st.session_state.chat_log = []
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "DialoGPT small"
if "selected_template" not in st.session_state:
    st.session_state.selected_template = "–û–±—ã—á–Ω—ã–π"
if "language" not in st.session_state:
    st.session_state.language = "–†—É—Å—Å–∫–∏–π"

st.title("BOtCH Chatbot with Extended Features")

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
selected_model = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å DialoGPT", list(model_options.keys()), index=list(model_options.keys()).index(st.session_state.selected_model))
if selected_model != st.session_state.selected_model:
    st.session_state.selected_model = selected_model
    st.session_state.history = None
    st.session_state.chat_log = []
    st.experimental_rerun()

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
tokenizer, model = load_model(model_options[st.session_state.selected_model])

# –í—ã–±–æ—Ä —à–∞–±–ª–æ–Ω–∞ –æ—Ç–≤–µ—Ç–∞
selected_template = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–∞", list(custom_templates.keys()), index=list(custom_templates.keys()).index(st.session_state.selected_template))
st.session_state.selected_template = selected_template

# –Ø–∑—ã–∫ (–ø—Ä–æ—Å—Ç–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏, –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
language = st.selectbox("–Ø–∑—ã–∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞", ["–†—É—Å—Å–∫–∏–π", "English"], index=0 if st.session_state.language == "–†—É—Å—Å–∫–∏–π" else 1)
st.session_state.language = language

# –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —Å —Ç–∞–π–º–∏–Ω–≥–æ–º
def generate_response(history, user_input, max_length=1000):
    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')
    bot_input_ids = torch.cat([history, new_user_input_ids], dim=-1) if history is not None else new_user_input_ids
    start_time = time.time()
    output_ids = model.generate(bot_input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)
    gen_time = time.time() - start_time
    response_ids = output_ids[:, bot_input_ids.shape[-1]:]
    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)
    return output_ids, response_text, gen_time

# –í–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
user_input = st.text_input("–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:", "")

# –ö–Ω–æ–ø–∫–∏
col1, col2, col3 = st.columns(3)
with col1:
    send_pressed = st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å")
with col2:
    clear_pressed = st.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é")
with col3:
    save_pressed = st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–≥ –≤ —Ñ–∞–π–ª")

if clear_pressed:
    st.session_state.history = None
    st.session_state.chat_log = []
    st.experimental_rerun()

if send_pressed and user_input.strip() != "":
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω –∫ –≤–æ–ø—Ä–æ—Å—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∏–ª—è)
    styled_input = custom_templates[st.session_state.selected_template](user_input)
    st.session_state.history, bot_response, gen_time = generate_response(st.session_state.history, styled_input)
    st.session_state.chat_log.append(("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", user_input))
    st.session_state.chat_log.append(("–ë–û–¢", bot_response))
    st.session_state.chat_log.append(("–ú–µ—Ç—Ä–∏–∫–∞", f"–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {gen_time:.2f} —Å–µ–∫, –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(bot_response)} —Å–∏–º–≤–æ–ª–æ–≤"))

if st.session_state.chat_log:
    for speaker, text in st.session_state.chat_log:
        if speaker == "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å":
            st.markdown(f"**{speaker}:** {text}")
        elif speaker == "–ë–û–¢":
            st.markdown(f"<div style='color: green'><b>{speaker}:</b> {text}</div>", unsafe_allow_html=True)
        else:  # –º–µ—Ç—Ä–∏–∫–∞
            st.markdown(f"<i style='color: gray; font-size: small;'>{text}</i>", unsafe_allow_html=True)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥ –≤ —Ñ–∞–π–ª
if save_pressed:
    log_text = ""
    for speaker, text in st.session_state.chat_log:
        log_text += f"{speaker}: {text}\n"
    with open("chat_log.txt", "w", encoding="utf-8") as f:
        f.write(log_text)
    st.success("–õ–æ–≥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ chat_log.txt")
    # –î–ª—è Colab –º–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    try:
        from google.colab import files
        files.download("chat_log.txt")
    except ImportError:
        pass